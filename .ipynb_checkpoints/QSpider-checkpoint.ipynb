{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dftrain=pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "dfdev=pd.read_csv(\"valid.tsv\", sep=\"\\t\")\n",
    "dftest=pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dftrain1=pd.read_csv(\"trainquestiontype.csv\")\n",
    "dfdev1=pd.read_csv(\"validquestiontype.csv\")\n",
    "dftest1=pd.read_csv(\"testquestiontype.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8588, 7)\n",
      "(302, 7)\n",
      "(230, 7)\n",
      "(17176, 14)\n",
      "(604, 14)\n",
      "(460, 14)\n"
     ]
    }
   ],
   "source": [
    "print(dftrain.shape)\n",
    "print(dfdev.shape)\n",
    "print(dftest.shape)\n",
    "print(dftrain1.shape)\n",
    "print(dfdev1.shape)\n",
    "print(dftest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[1, 1, 0, 0, 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[0, 0, 1, 0, 1]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "ytrainlist=list(dftrain['is_duplicate'])\n",
    "ydevlist=list(dfdev['is_duplicate'])\n",
    "ytestlist=list(dftest['is_duplicate'])\n",
    "ytrain=np.zeros((len(ytrainlist),2))\n",
    "ydev=np.zeros((len(ydevlist),2))\n",
    "ytest=np.zeros((len(ytestlist),2))\n",
    "for i in range(len(ytrainlist)):\n",
    "    if ytrainlist[i]==0:\n",
    "        ytrain[i][0]=1\n",
    "    else:\n",
    "        ytrain[i][1]=1\n",
    "for i in range(len(ydevlist)):\n",
    "    if ydevlist[i]==0:\n",
    "        ydev[i][0]=1\n",
    "    else:\n",
    "        ydev[i][1]=1\n",
    "for i in range(len(ytestlist)):\n",
    "    if ytestlist[i]==0:\n",
    "        ytest[i][0]=1\n",
    "    else:\n",
    "        ytest[i][1]=1\n",
    "print(ytrain[5:10])\n",
    "print(ytrainlist[5:10])\n",
    "print(ydev[5:10])\n",
    "print(ydevlist[5:10])\n",
    "print(ytest[5:10])\n",
    "print(ytestlist[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = dftrain1.drop('id', axis=1)\n",
    "Xdev = dfdev1.drop('id', axis=1)\n",
    "Xtest = dftest1.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=np.array(Xtrain)\n",
    "xtrain1=xtrain[:int(xtrain.shape[0]/2)]\n",
    "xtrain2=xtrain[int(xtrain.shape[0]/2):]\n",
    "xtrain3=np.hstack((xtrain1,xtrain2))\n",
    "xdev=np.array(Xdev)\n",
    "xdev1=xdev[:int(xdev.shape[0]/2)]\n",
    "xdev2=xdev[int(xdev.shape[0]/2):]\n",
    "xdev3=np.hstack((xdev1,xdev2))\n",
    "xtest=np.array(Xtest)\n",
    "xtest1=xtest[:int(xtest.shape[0]/2)]\n",
    "xtest2=xtest[int(xtest.shape[0]/2):]\n",
    "xtest3=np.hstack((xtest1,xtest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8890"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=np.vstack((xtrain3,xdev3))\n",
    "X_train.shape\n",
    "y_trainlist=ytrainlist+ydevlist\n",
    "y_train=np.vstack((ytrain,ydev))\n",
    "len(y_trainlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8890, 26)\n",
      "(230, 26)\n"
     ]
    }
   ],
   "source": [
    "Xtrain5=np.copy(X_train)\n",
    "for i in range(len(Xtrain5)):\n",
    "    for j in range(26):\n",
    "        if Xtrain5[i][j]<0.5:\n",
    "            Xtrain5[i][j]=0\n",
    "        else:\n",
    "            Xtrain5[i][j]=1\n",
    "Xtest5=np.copy(xtest3)\n",
    "for i in range(len(Xtest5)):\n",
    "    for j in range(26):\n",
    "        if Xtest5[i][j]<0.5:\n",
    "            Xtest5[i][j]=0\n",
    "        else:\n",
    "            Xtest5[i][j]=1\n",
    "print(Xtrain5.shape)\n",
    "print(Xtest5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Linear SVM\": SVC(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=10000),\n",
    "    \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=1000),\n",
    "    \"Neural Net\": MLPClassifier(alpha = 0.01,hidden_layer_sizes=(40, 20, 20), max_iter=50000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    #\"AdaBoost\": AdaBoostClassifier(),\n",
    "    #\"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    #\"Gaussian Process\": GaussianProcessClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "# clf.fit(X_train,np.array(y_trainlist))\n",
    "# print(clf.score(X_train,np.array(y_trainlist)))\n",
    "# print(clf.best_params_)\n",
    "# result=clf.predict(xtest3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 5, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "    is because it is very easy to save the whole dictionary with the pickle module.\n",
    "    \n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n",
    "        t_start = time.clock()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.clock()\n",
    "        \n",
    "        t_diff = t_end - t_start\n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        Y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'train_time': t_diff, 'predictions':Y_pred}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models\n",
    "\n",
    "\n",
    "\n",
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),4)), columns = ['classifier', 'train_score', 'test_score', 'train_time'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakhar/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Logistic Regression in 0.03 s\n",
      "trained Nearest Neighbors in 0.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakhar/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Linear SVM in 4.97 s\n"
     ]
    }
   ],
   "source": [
    "dict_models = batch_classify(Xtrain5, np.array(y_trainlist), Xtest5, np.array(ytestlist), no_classifiers = 11)\n",
    "display_dict_models(dict_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2=np.array(dict_models['Gradient Boosting Classifier']['predictions'])\n",
    "list1=np.arange(1,len(list2)+1)\n",
    "list1=list1.reshape(-1,1)\n",
    "list2=list2.reshape(-1,1)\n",
    "list1=np.hstack((list1,list2))\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "df=pd.DataFrame(list1)\n",
    "df.columns = ['pair_id','label']\n",
    "import csv\n",
    "df.to_csv(\"submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "y=dict_models['Gradient Boosting Classifier']['predictions']\n",
    "print(confusion_matrix(y, ytestlist))\n",
    "print(accuracy_score(y, ytestlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
